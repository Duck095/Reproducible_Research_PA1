---
title: "R Notebook"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

LAB_5_6_TranDucNgocLong_SE173342

## M·ª•c ti√™u h·ªçc t·∫≠p

Sau khi ho√†n th√†nh b√†i lab n√†y, c√°c b·∫°n s·∫Ω c√≥ th·ªÉ:

1.  Hi·ªÉu c√°c kh√°i ni·ªám c∆° b·∫£n v·ªÅ m√¥ h√¨nh tuy·∫øn t√≠nh
2.  S·ª≠ d·ª•ng c√°c h√†m `lm()` v√† `loess()` ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh
3.  Tr·ª±c quan h√≥a m√¥ h√¨nh v√† d·ª± ƒëo√°n
4.  Ph√¢n t√≠ch residual ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥ h√¨nh
5.  X·ª≠ l√Ω c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu
6.  S·ª≠ d·ª•ng c√°c h√†m h·ªó tr·ª£ nh∆∞ `add_predictions()` v√† `add_residuals()`

```{r}
# install.packages(c("tidyverse", "modelr"))

library("tidyverse")
library("modelr")

```

```{r}
# Thi·∫øt l·∫≠p x·ª≠ l√Ω gi√° tr·ªã missing
options(na.action = na.warn)
```

## Ph·∫ßn 1: M√¥ h√¨nh ƒë∆°n gi·∫£n v√† ·∫£nh h∆∞·ªüng c·ªßa outliers (25 ƒëi·ªÉm)

### 1.1. T·∫°o d·ªØ li·ªáu m√¥ ph·ªèng v·ªõi Student's t-distribution

```{r}
# T·∫°o d·ªØ li·ªáu v·ªõi t-distribution
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

print(sim1a)

```

The **t-distribution** (c√≤n g·ªçi l√† **Student‚Äôs t-distribution**) th∆∞·ªùng
ƒë∆∞·ª£c d√πng trong th·ªëng k√™, ƒë·∫∑c bi·ªát khi:

-   C·ª° m·∫´u nh·ªè (**n \< 30**).

-   Ph∆∞∆°ng sai d√¢n s·ªë (œÉ¬≤) ch∆∞a bi·∫øt.

-   Ta thay th·∫ø ƒë·ªô l·ªách chu·∫©n th·∫≠t b·∫±ng **∆∞·ªõc l∆∞·ª£ng t·ª´ m·∫´u**.

-   N·∫øu Z‚àºN(0,1) (chu·∫©n h√≥a) v√† V‚àºœá2(ŒΩ) (ph√¢n ph·ªëi Chi-square v·ªõi ŒΩ b·∫≠c
    t·ª± do), ƒë·ªôc l·∫≠p nhau, th√¨:

```         
T=Z/sqrt(ŒΩT)
```

-   s·∫Ω tu√¢n theo **t-distribution** v·ªõi ŒΩ b·∫≠c t·ª± do (degrees of freedom,
    df).

```{r}
# V·∫Ω bi·ªÉu ƒë·ªì c√≥ ƒë∆∞·ªùng h·ªìi quy
ggplot(sim1a, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "M√¥ h√¨nh tuy·∫øn t√≠nh v·ªõi t-distribution",
       x = "X", y = "Y")
```

### 1.2. S·ª≠ d·ª•ng Ph∆∞∆°ng ph√°p tr·ª±c quan ƒë·ªÉ So s√°nh

```{r}
# H√†m t·∫°o d·ªØ li·ªáu v·ªõi t-distribution
simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    .id = i
  )
}

# T·∫°o 12 b·ªô d·ªØ li·ªáu m√¥ ph·ªèng
sims <-  map_dfr(1:12, simt)

```

```{r}
# V·∫Ω bi·ªÉu ƒë·ªì v·ªõi 12 panel
ggplot(sims, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red", se = FALSE) +
  facet_wrap(~.id, ncol = 4) +
  labs(title = "12 m√¥ ph·ªèng v·ªõi t-distribution")
```

### 1.3. So s√°nh v·ªõi normal distribution

```{r}
# H√†m t·∫°o d·ªØ li·ªáu v·ªõi normal distribution
sim_norm <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

# T·∫°o 12 b·ªô d·ªØ li·ªáu v·ªõi normal distribution
simdf_norm <-  map_dfr(1:12, sim_norm)

```

```{r}
# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
ggplot(simdf_norm, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red", se = FALSE) +
  facet_wrap(~.id, ncol = 4) +
  labs(title = "12 m√¥ ph·ªèng v·ªõi Normal distribution")
```

### 1.4. So s√°nh ph√¢n ph·ªëi

#### M·ª•c ti√™u

-   Tr·ª±c quan h√≥a s·ª± kh√°c bi·ªát gi·ªØa Normal distribution v√† Student-t
    distribution.

-   Ch·ª©ng minh: Student-t c√≥ tail d√†y h∆°n, t·ª©c l√† x√°c su·∫•t g·∫∑p outlier
    cao h∆°n.

\*\* Th·ª±c hi·ªán v·∫Ω Normal distribution v√† t-distributionƒë·ªÉ ƒë√°nh gi√°:\*\*

-   ƒê∆∞·ªùng Normal distribution (chu·∫©n) c√≥ d·∫°ng ‚Äúchu√¥ng‚Äù, t·∫≠p trung nhi·ªÅu
    ·ªü trung t√¢m (x = 0) v√† tail m·ªèng.

-   ƒê∆∞·ªùng t-distribution v·ªõi df = 2 c√≥ d·∫°ng t∆∞∆°ng t·ª± nh∆∞ng ƒëu√¥i d√†y h∆°n
    (heavy tails) ‚Üí x√°c su·∫•t quan s√°t ƒë∆∞·ª£c gi√° tr·ªã xa trung t√¢m cao h∆°n.

-   Khi df (degrees of freedom) tƒÉng, ph√¢n ph·ªëi t d·∫ßn d·∫ßn ti·ªám c·∫≠n
    Normal distribution.

```{r}
# V·∫Ω so s√°nh density c·ªßa t-distribution v√† normal distribution
tibble(
  x = seq(-5, 5, length.out = 100),
  normal = dnorm(x),
  student_t = dt(x, df = 2)
) %>%
  pivot_longer(-x, names_to="distribution", values_to="density") %>%
  ggplot(aes(x = x, y = density, colour = distribution)) +
  geom_line(size = 1.2) +
  labs(title = "So s√°nh Normal distribution v√† t-distribution",
       x = "Gi√° tr·ªã", y = "M·∫≠t ƒë·ªô x√°c su·∫•t")

# T√≠nh x√°c su·∫•t tail
# X√°c su·∫•t > 2 v·ªõi Normal distribution:

cat("x√°c su·∫•t > 2 v·ªõi Normal distribution:", pnorm(2, lower.tail = FALSE), "\n")
# X√°c su·∫•t > 2 v·ªõi t-distribution (df=2):
cat("x√°c su·∫•t > 2 v·ªõi t-distribution (df=2):", pt(2, df = 2, lower.tail = FALSE), "\n")

```

#### S·ª¨ d·ª•ng t-distribution khi:

-   D·ªØ li·ªáu √≠t (small sample size).

-   C·∫ßn m√¥ h√¨nh robust h∆°n v·ªõi outlier.

-   Trong Bayesian inference, noise th∆∞·ªùng ƒë∆∞·ª£c m√¥ h√¨nh h√≥a b·∫±ng
    Student-t thay v√¨ Gaussian.

##### M·ª•c ti√™u so s√°nh:

-   ∆Ø·ªõc l∆∞·ª£ng tham s·ªë: v·ªõi d·ªØ li·ªáu √≠t (sample nh·ªè), ta hay d√πng
    t-distribution ƒë·ªÉ ph·∫£n √°nh ƒë·ªô b·∫•t ƒë·ªãnh cao h∆°n so v·ªõi Normal.

-   Ph√°t hi·ªán outlier: t-distribution v·ªõi heavy tails c√≥ th·ªÉ ch·ªëng l·∫°i
    ·∫£nh h∆∞·ªüng c·ªßa outlier t·ªët h∆°n trong m·ªôt s·ªë m√¥ h√¨nh (v√≠ d·ª• Bayesian
    inference).

-   Regularization: √Ω t∆∞·ªüng ‚Äúƒëu√¥i d√†y‚Äù g·∫ßn gi·ªëng L1 penalty (cho ph√©p
    gi√° tr·ªã l·ªõn hi·∫øm khi x·∫£y ra) so v·ªõi Gaussian prior (L2 penalty).

**C√¢u 1: (8 ƒëi·ªÉm)** T·∫°i sao t-distribution v·ªõi df=2 t·∫°o ra nhi·ªÅu gi√° tr·ªã
c·ª±c tr·ªã (outliers) h∆°n so v·ªõi normal distribution? H√£y gi·∫£i th√≠ch v√† so
s√°nh x√°c su·∫•t P(X \> 2) c·ªßa hai ph√¢n ph·ªëi n√†y.

**ƒê√°p √°n:**

```         
T-distribution v·ªõi df=2 c√≥ ƒëu√¥i d√†y h∆°n normal distribution, n√™n n√≥ cho nhi·ªÅu gi√° tr·ªã c·ª±c tr·ªã (outliers) h∆°n.
V√≠ d·ª•, ùëÉ(ùëã>2) v·ªõi t(df=2) l·ªõn h∆°n nhi·ªÅu so v·ªõi normal(0,1) (kho·∫£ng 0.1 so v·ªõi 0.023).
```

**C√¢u 2: (8 ƒëi·ªÉm)** Quan s√°t 12 bi·ªÉu ƒë·ªì m√¥ ph·ªèng v·ªõi t-distribution v√†
normal distribution. M√¥ h√¨nh n√†o c√≥ ƒë∆∞·ªùng h·ªìi quy ·ªïn ƒë·ªãnh h∆°n v√† t·∫°i
sao?

**ƒê√°p √°n:**

```         
M√¥ h√¨nh v·ªõi normal distribution c√≥ ƒë∆∞·ªùng h·ªìi quy ·ªïn ƒë·ªãnh h∆°n v√¨ √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers.
M√¥ h√¨nh v·ªõi t-distribution dao ƒë·ªông nhi·ªÅu h∆°n do c√°c outliers l√†m ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn k·∫øt qu·∫£.
```

**C√¢u 3: (9 ƒëi·ªÉm)** N·∫øu b·∫°n c√≥ m·ªôt dataset th·ª±c t·∫ø v·ªõi nhi·ªÅu outliers,
b·∫°n s·∫Ω ch·ªçn ph∆∞∆°ng ph√°p n√†o ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh? ƒê·ªÅ xu·∫•t √≠t nh·∫•t 2 gi·∫£i
ph√°p.

**ƒê√°p √°n:**

```         
N·∫øu dataset c√≥ nhi·ªÅu outliers, ta c√≥ th·ªÉ:

D√πng h·ªìi quy robust (nh∆∞ M-estimator) ƒë·ªÉ gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa outliers.

Lo·∫°i ho·∫∑c ƒëi·ªÅu ch·ªânh outliers tr∆∞·ªõc khi x√¢y d·ª±ng m√¥ h√¨nh.
```

## Ph·∫ßn 2: C√°c ph∆∞∆°ng ph√°p ƒëo kho·∫£ng c√°ch kh√°c nhau (20 ƒëi·ªÉm)

T√¨m hi·ªÉu kh√°i ni·ªám t√≠nh to√°n ƒë·ªÉ fit m·ªôt ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh ƒë∆°n
gi·∫£n:

$$ \hat{y} = Œ≤0‚Äã+Œ≤1‚Äãx $$ thay v√¨ d√πng c√¥ng th·ª©c `lm(y ~ x)` (OLS ‚Äì
ordinary least squares), c√°c b·∫°n t√¨m hi·ªÉu **h√†m ƒëo sai s·ªë (loss
function)** r·ªìi d√πng `optim()` ƒë·ªÉ t√¨m tham s·ªë t·ªëi ∆∞u.

-   2 lo·∫°i loss function:

------------------------------------------------------------------------

### 1. MAD (Mean Absolute Distance)

$$
L(\beta_0, \beta_1) = \frac{1}{n} \sum |y_i - \hat{y}_i|
$$

------------------------------------------------------------------------

### 2. RMSD (Root Mean Squared Distance)

$$
L(\beta_0, \beta_1) = \frac{1}{n} \sum (y_i - \hat{y}_i)^2
$$

### 2.1. Mean Absolute Deviation

```{r}
# H√†m ƒëo kho·∫£ng c√°ch b·∫±ng Mean Absolute Distance
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}

# H√†m d·ª± ƒëo√°n
make_prediction <- function(mod, data) {
mod [1] + mod [2] * data$x
}

```

-   mod l√† vector tham s·ªë [Œ≤0, Œ≤1].

-   data\$x l√† d·ªØ li·ªáu ƒë·∫ßu v√†o. - \>Tr·∫£ v·ªÅ gi√° tr·ªã d·ª± ƒëo√°n $$ \hat{y}$$

```{r}
# T√¨m tham s·ªë t·ªëi ∆∞u v·ªõi MAD
best_mad <- optim(c(0, 0), measure_distance, data = sim1a)
cat("Tham s·ªë t·ªëi ∆∞u v·ªõi MAD:", best_mad$par, "\n")

# H√†m ƒëo kho·∫£ng c√°ch b·∫±ng Root Mean Squared Distance( T√≠nh sai s·ªë tuy·ªát ƒë·ªëi gi·ªØa gi√° tr·ªã th·∫≠t y v√† d·ª± ƒëo√°n ≈∑.)

# D√πng trung b√¨nh ƒë·ªÉ c√≥ gi√° tr·ªã t·ªïng qu√°t.
measure_distance_ls <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  sqrt(mean(diff^2))
}
```

-   optim(c(0, 0), ...) kh·ªüi t·∫°o Œ≤0 = 0, Œ≤1 = 0.
-   T√¨m c·∫∑p [Œ≤0, Œ≤1] sao cho h√†m MAD nh·ªè nh·∫•t.

```{r}
# T√¨m tham s·ªë t·ªëi ∆∞u v·ªõi RMSD
best_rmsd <- optim(c(0, 0), measure_distance_ls, data = sim1a)
cat("Tham s·ªë t·ªëi ∆∞u v·ªõi RMSD:", best_rmsd$par, "\n")
```

-   Gi·ªëng MSE (Mean Squared Error), ch·ªâ kh√°c l√† l·∫•y cƒÉn b·∫≠c 2.

-   Ph·∫°t n·∫∑ng c√°c sai s·ªë l·ªõn ‚Üí nh·∫°y c·∫£m v·ªõi outliers.

### C√¢u h·ªèi v√† ƒë√°p √°n - Ph·∫ßn 2 (20 ƒëi·ªÉm)

**C√¢u 1: (7 ƒëi·ªÉm)** Gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa Mean Absolute Distance
(MAD) v√† Root Mean Squared Distance (RMSD). Khi n√†o n√™n s·ª≠ d·ª•ng MAD?

**ƒê√°p √°n:**

MAD (Mean Absolute Distance) l√† trung b√¨nh gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa sai s·ªë,
c√≤n RMSD (Root Mean Squared Distance) l√† cƒÉn b·∫≠c hai c·ªßa trung b√¨nh b√¨nh
ph∆∞∆°ng sai s·ªë.

MAD √≠t nh·∫°y c·∫£m v·ªõi outliers v√¨ ch·ªâ t√≠nh tr·ªã tuy·ªát ƒë·ªëi, kh√¥ng b√¨nh
ph∆∞∆°ng sai s·ªë.

N√™n d√πng MAD khi d·ªØ li·ªáu c√≥ outliers ho·∫∑c mu·ªën gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa c√°c
sai s·ªë c·ª±c l·ªõn.

**C√¢u 2: (6 ƒëi·ªÉm)** Trong code tr√™n, t·∫°i sao h√†m `optim()` c√≥ th·ªÉ cho
k·∫øt qu·∫£ kh√°c nhau t√πy thu·ªôc v√†o gi√° tr·ªã kh·ªüi t·∫°o?

**ƒê√°p √°n:**

H√†m optim() s·ª≠ d·ª•ng thu·∫≠t to√°n t·ªëi ∆∞u d·ª±a tr√™n ƒëi·ªÉm kh·ªüi t·∫°o.

Gi√° tr·ªã kh·ªüi t·∫°o kh√°c nhau c√≥ th·ªÉ d·∫´n ƒë·∫øn t√¨m ƒë∆∞·ª£c c·ª±c tr·ªã c·ª•c b·ªô kh√°c
nhau, n√™n k·∫øt qu·∫£ c≈©ng kh√°c nhau.

**C√¢u 3: (7 ƒëi·ªÉm)** Vi·∫øt c√¥ng th·ª©c to√°n h·ªçc cho c·∫£ MAD v√† RMSD. Gi·∫£i
th√≠ch t·∫°i sao RMSD "ph·∫°t" outliers n·∫∑ng h∆°n MAD.

**ƒê√°p √°n:**

-   C√¥ng th·ª©c: $$
    \text{MAD} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
    $$ $$
    \text{RMSD} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
    $$

-   RMSD "ph·∫°t" outliers n·∫∑ng h∆°n v√¨ sai s·ªë ƒë∆∞·ª£c b√¨nh ph∆∞∆°ng, l√†m cho
    c√°c l·ªói l·ªõn c√≥ ·∫£nh h∆∞·ªüng l·ªõn h∆°n trong t√≠nh to√°n.

## Ph·∫ßn 3: Tr·ª±c quan h√≥a m√¥ h√¨nh (25 ƒëi·ªÉm)

### 3.1. So s√°nh lm() v√† loess()

```{r}
# S·ª≠ d·ª•ng d·ªØ li·ªáu sim1 c√≥ s·∫µn trong modelr
# X√¢y d·ª±ng m√¥ h√¨nh
sim1_loess <- loess(y ~  x, data = sim1)
sim1_lm <- lm(y ~  x, data= sim1)
```

```{r}
# Th√™m predictions v√† residuals
sim1_extended <- sim1 %>%
  add_residuals(sim1_lm, var = "resid_lm") %>%
  add_predictions(sim1_lm, var = "pred_lm") %>%
  add_residuals(sim1_loess, var = "resid_loess") %>%
  add_predictions(sim1_loess, var = "pred_loess")

```

```{r}
# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
ggplot(sim1_extended, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred_lm), color = "red", size = 1) +
  geom_line(aes(y = pred_loess), color = "blue", size = 1) +
  labs(title = "So s√°nh Linear Model (ƒë·ªè) v√† LOESS (xanh)",
       x = "X", y = "Y")
```

### `lm()` ‚Äì **Linear Model**

-   D√πng ƒë·ªÉ x√¢y d·ª±ng **m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh** (linear regression).

-   Gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa bi·∫øn ƒë·ªôc l·∫≠p x v√† bi·∫øn ph·ª• thu·ªôc y l√†
    **tuy·∫øn t√≠nh** ho·∫∑c c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c b·∫±ng c√°c bi·∫øn bi·∫øn ƒë·ªïi
    (nh∆∞ ƒëa th·ª©c).

### `loess()` ‚Äì **Local Regression (phi tuy·∫øn)**

-   L√† **m√¥ h√¨nh h·ªìi quy c·ª•c b·ªô** (local regression, local polynomial
    regression).

-   Kh√¥ng gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá tuy·∫øn t√≠nh to√†n c·ª•c, m√† t√¨m **ƒë∆∞·ªùng cong
    tr∆°n** m√¥ t·∫£ d·ªØ li·ªáu t·ªët h∆°n khi quan h·ªá gi·ªØa x v√† y l√† phi tuy·∫øn.

## **Student‚Äôs t-distribution**

The **t-distribution** (c√≤n g·ªçi l√† **Student‚Äôs t-distribution**)

N√≥ th∆∞·ªùng ƒë∆∞·ª£c d√πng trong th·ªëng k√™ khi:

-   C·ª° m·∫´u nh·ªè (**n \< 30**).

-   Ph∆∞∆°ng sai d√¢n s·ªë (œÉ¬≤) ch∆∞a bi·∫øt.

-   Thay th·∫ø ƒë·ªô l·ªách chu·∫©n th·∫≠t b·∫±ng **∆∞·ªõc l∆∞·ª£ng t·ª´ m·∫´u**.

N·∫øu Z‚àºN(0,1) (chu·∫©n h√≥a) v√† V‚àºœá2 (ph√¢n ph·ªëi Chi-square v·ªõi ŒΩ b·∫≠c t·ª± do),
ƒë·ªôc l·∫≠p nhau, th√¨:

T=Z/sqrt(ŒΩT)

s·∫Ω tu√¢n theo **t-distribution** v·ªõi ŒΩ\nuŒΩ b·∫≠c t·ª± do (degrees of freedom,
df).

## **ƒê·∫∑c ƒëi·ªÉm**

-   **ƒê·ªëi x·ª©ng** quanh 0 (gi·ªëng ph√¢n ph·ªëi chu·∫©n)

-   **ƒêu√¥i d√†y h∆°n** ph√¢n ph·ªëi chu·∫©n ‚Üí cho ph√©p ‚Äúnhi·ªÅu bi·∫øn ƒë·ªông h∆°n‚Äù
    khi c·ª° m·∫´u nh·ªè

-   Khi s·ªë b·∫≠c t·ª± do ŒΩ‚Üí‚àû, ph√¢n ph·ªëi t ti·∫øn g·∫ßn ƒë·∫øn chu·∫©n chu·∫©n h√≥a
    N(0,1).

## **·ª®ng d·ª•ng**

1.  **Ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt**:

```         
-    Ki·ªÉm ƒë·ªãnh m·ªôt m·∫´u t-test.

-    Ki·ªÉm ƒë·ªãnh hai m·∫´u ƒë·ªôc l·∫≠p.

-    Ki·ªÉm ƒë·ªãnh c·∫∑p m·∫´u (paired t-test).
```

2.  **Kho·∫£ng tin c·∫≠y (Confidence Interval)** cho trung b√¨nh khi kh√¥ng
    bi·∫øt ph∆∞∆°ng sai.

### 3.2. Ph√¢n t√≠ch residuals

Trong h·ªìi quy, **residual** (s·ªë d∆∞, hay ph·∫ßn d∆∞) l√† **kho·∫£ng c√°ch d·ªçc**
gi·ªØa gi√° tr·ªã quan s√°t th·ª±c t·∫ø v√† gi√° tr·ªã d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh.

```{r}
# V·∫Ω residuals
ggplot(sim1_extended, aes(x = x)) +
  geom_ref_line(h = 0) +
  geom_point(aes(y = resid_lm), color = "red", alpha = 0.7) +
  geom_point(aes(y = resid_loess), color = "blue", alpha = 0.7) +
  labs(title = "So s√°nh Residuals: Linear Model (ƒë·ªè) vs LOESS (xanh)",
       x = "X", y = "Residuals")

```

### 3.3. S·ª≠ d·ª•ng gather_predictions() v√† spread_predictions()

gather_predictions() D√πng khi c√≥ nhi·ªÅu m√¥ h√¨nh v√† mu·ªën thu th·∫≠p (gather)
t·∫•t c·∫£ d·ª± ƒëo√°n v√†o m·ªôt c·ªôt duy nh·∫•t.

-   `.model` ‚Üí t√™n m√¥ h√¨nh.

<!-- -->

-   `.pred` ‚Üí gi√° tr·ªã d·ª± ƒëo√°n.

**`spread_predictions()`** Ng∆∞·ª£c l·∫°i v·ªõi gather. D√πng khi mu·ªën tr·∫£i
(spread) d·ª± ƒëo√°n c·ªßa nhi·ªÅu m√¥ h√¨nh th√†nh **nhi·ªÅu c·ªôt ri√™ng bi·ªát**.

.

```{r}
# T·∫°o grid cho d·ª± ƒëo√°n
grid <- sim1 %>%
  data_grid(x)
```

```{r}
# S·ª≠ d·ª•ng gather_predictions
grid_gather <- grid %>% gather_predictions(sim1_lm, sim1_loess)

print("K·∫øt qu·∫£ gather_predictions:")
head(grid_gather)
```

```{r}
# S·ª≠ d·ª•ng spread_predictions
grid_spread <- grid %>% spread_predictions (sim1_lm, sim1_loess)

print("K·∫øt qu·∫£ spread_predictions:")
head(grid_spread)
```

```{r}
# V·∫Ω bi·ªÉu ƒë·ªì t·ª´ gather_predictions
ggplot(sim1, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred, color = model), data = grid_gather, size = 1) +
  labs(title = "Multiple Models using gather_predictions()")
```

### C√¢u h·ªèi v√† ƒë√°p √°n - Ph·∫ßn 3 (25 ƒëi·ªÉm)

**C√¢u 1: (8 ƒëi·ªÉm)** So s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm c·ªßa `lm()` v√† `loess()`. Khi
n√†o n√™n s·ª≠ d·ª•ng t·ª´ng ph∆∞∆°ng ph√°p?

**ƒê√°p √°n:**

lm(): M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh, nhanh, d·ªÖ hi·ªÉu, ph√π h·ª£p khi m·ªëi quan
h·ªá gi·ªØa bi·∫øn ph·ª• thu·ªôc v√† ƒë·ªôc l·∫≠p l√† tuy·∫øn t√≠nh.

loess(): M√¥ h√¨nh h·ªìi quy phi tuy·∫øn t√≠nh, linh ho·∫°t, x·ª≠ l√Ω d·ªØ li·ªáu phi
tuy·∫øn t·ªët nh∆∞ng ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn.

Khi n√†o d√πng: D√πng lm() khi d·ªØ li·ªáu c√≥ xu h∆∞·ªõng tuy·∫øn t√≠nh; d√πng loess()
khi d·ªØ li·ªáu c√≥ quan h·ªá ph·ª©c t·∫°p, kh√¥ng tuy·∫øn t√≠nh.

**C√¢u 2: (9 ƒëi·ªÉm)** Gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa `add_predictions()`,
`gather_predictions()`, v√† `spread_predictions()`. Cho v√≠ d·ª• khi n√†o s·ª≠
d·ª•ng t·ª´ng h√†m.

**ƒê√°p √°n:**

add_predictions(): Th√™m c·ªôt d·ª± ƒëo√°n v√†o m·ªôt dataframe c√≥ d·ªØ li·ªáu g·ªëc.

gather_predictions(): Gom d·ª± ƒëo√°n t·ª´ nhi·ªÅu m√¥ h√¨nh kh√°c nhau th√†nh d·∫°ng
‚Äúd√†i‚Äù ƒë·ªÉ so s√°nh.

spread_predictions(): Bi·∫øn d·ªØ li·ªáu d·ª± ƒëo√°n t·ª´ d·∫°ng ‚Äúd√†i‚Äù th√†nh ‚Äúr·ªông‚Äù ƒë·ªÉ
so s√°nh m√¥ h√¨nh.

V√≠ d·ª•:

D√πng add_predictions() ƒë·ªÉ th√™m d·ª± ƒëo√°n v√†o d·ªØ li·ªáu g·ªëc.

D√πng gather_predictions() khi c√≥ nhi·ªÅu m√¥ h√¨nh v√† mu·ªën so s√°nh chung.

D√πng spread_predictions() khi mu·ªën xem d·ª± ƒëo√°n t·ª´ng m√¥ h√¨nh b√™n c·∫°nh
nhau.

**C√¢u 3: (8 ƒëi·ªÉm)** T·∫°i sao vi·ªác v·∫Ω `geom_ref_line(h = 0)` quan tr·ªçng
khi ph√¢n t√≠ch residuals? Residuals t·ªët c·∫ßn c√≥ nh·ªØng t√≠nh ch·∫•t g√¨?

**ƒê√°p √°n:**

V·∫Ω geom_ref_line(h = 0) gi√∫p d·ªÖ nh·∫≠n bi·∫øt c√°c residuals l·ªách kh·ªèi 0, th·ªÉ
hi·ªán sai s·ªë d·ª± ƒëo√°n.

Residuals t·ªët c·∫ßn: ph√¢n b·ªë ng·∫´u nhi√™n quanh 0, kh√¥ng c√≥ xu h∆∞·ªõng, ph∆∞∆°ng
sai ƒë·ªìng nh·∫•t, kh√¥ng c√≥ outliers l·ªõn.

## Ph·∫ßn 4: L√†m vi·ªác v·ªõi d·ªØ li·ªáu categorical (30 ƒëi·ªÉm)

### 4.1. Ph√¢n t√≠ch sim2 (m·ªôt bi·∫øn categorical)

```{r}
# Xem d·ªØ li·ªáu sim2
print("D·ªØ li·ªáu sim2:")
head(sim2)
```

```{r}
# M√¥ h√¨nh c√≥ intercept
mod2_with_intercept <- lm(y ~ x, data = sim2)

mod2_with_intercept
```

y \~ x ‚Üí m√¥ h√¨nh c√≥ intercept (Œ≤0 + Œ≤1x).

```{r}
# M√¥ h√¨nh kh√¥ng c√≥ intercept
mod2_without_intercept <-  lm(y ~ x - 1, data = sim2)

mod2_without_intercept
```

y \~ x - 1 ‚Üí m√¥ h√¨nh kh√¥ng c√≥ intercept (ch·ªâ Œ≤1x).

##### Kh√°c bi·ªát:

-   C√≥ intercept: ƒë∆∞·ªùng h·ªìi quy c√≥ th·ªÉ d·ªãch chuy·ªÉn l√™n/xu·ªëng tr·ª•c tung.

-   Kh√¥ng intercept: ƒë∆∞·ªùng h·ªìi quy b·∫Øt bu·ªôc ƒëi qua g·ªëc (0,0).

```{r}
# So s√°nh predictions
grid2 <-sim2 %>% data_grid(x) %>% spread_predictions (mod2_with_intercept, mod2_without_intercept)

print("So s√°nh predictions:")
grid2
```

-   data_grid(x) ‚Üí t·∫°o l∆∞·ªõi gi√° tr·ªã x ƒë·ªÉ t√≠nh prediction.

-   spread_predictions() ‚Üí t√≠nh gi√° tr·ªã d·ª± ƒëo√°n t·ª´ c·∫£ 2 m√¥ h√¨nh tr√™n
    c√πng t·∫≠p grid.

    -   mod2_with_intercept: d·ª± ƒëo√°n d·ªãch chuy·ªÉn theo intercept.

    -   mod2_without_intercept: d·ª± ƒëo√°n ƒëi qua g·ªëc (0,0).

Trong th·ª±c t·∫ø khi b·ªè intercept khi b·∫°n ch·∫Øc ch·∫Øn d·ªØ li·ªáu c√≥ quan h·ªá
tuy·∫øn t√≠nh ƒëi qua g·ªëc (v√≠ d·ª•: chi·ªÅu cao = 0 ‚Üí c√¢n n·∫∑ng = 0).

### 4.2. Ph√¢n t√≠ch model matrix

```{r}
# Xem model matrix c·ªßa sim3 (continuous * categorical)
x3 <- model_matrix(y ~ x1 * x2, data = sim3)
print("Model matrix c·ªßa x1 * x2 (sim3):")
head(x3)
```

```{r}
# Xem model matrix c·ªßa sim4 (continuous * continuous)
x4 <- model_matrix(y ~ x1 * x2, data = sim4)
print("Model matrix c·ªßa x1 * x2 (sim4):")
head(x4)
```

-   Model matrix: tr·ª±c quan d·ªØ li·ªáu ƒë∆∞·ª£c bi·∫øn ƒë·ªïi th√†nh features tr∆∞·ªõc
    khi training. Trong ML g·ªçi l√† feature engineering / encoding.

-   th√™m feature phi tuy·∫øn (non-linear feature) ƒë·ªÉ m√¥ h√¨nh tuy·∫øn t√≠nh
    tr·ªü n√™n m·∫°nh h∆°n.

### 4.3. So s√°nh c√°c m√¥ h√¨nh tr√™n sim4

```{r}
# X√¢y d·ª±ng hai m√¥ h√¨nh
mod1_sim4 <- lm(y ~ x1 + x2, data = sim4)
mod2_sim4 <- lm(y ~ x1 * x2, data = sim4)
```

```{r}
mod1_sim4
mod2_sim4
```

```{r}
# Th√™m residuals
sim4_mods <- gather_residuals(sim4, mod1_sim4, mod2_sim4)
```

```{r}
# V·∫Ω frequency plot c·ªßa residuals
ggplot(sim4_mods, aes(x = resid, colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug() +
  labs(title = "Ph√¢n ph·ªëi Residuals c·ªßa hai m√¥ h√¨nh")
```

```{r}
# V·∫Ω frequency plot c·ªßa absolute residuals
ggplot(sim4_mods, aes(x = abs(resid), colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug() +
  labs(title = "Ph√¢n ph·ªëi |Residuals| c·ªßa hai m√¥ h√¨nh")
```

```{r}
# So s√°nh standard deviation c·ªßa residuals
sim4_comparison <- sim4_mods %>%
  group_by(model) %>%
  summarise(
    mean_resid = mean(resid),
    sd_resid = sd(resid),
    mad_resid = mad(resid),
    .groups = 'drop')

print("So s√°nh ch·∫•t l∆∞·ª£ng m√¥ h√¨nh:")
sim4_comparison
```

### C√¢u h·ªèi v√† ƒë√°p √°n - Ph·∫ßn 4 (30 ƒëi·ªÉm)

**C√¢u 1: (10 ƒëi·ªÉm)** Gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa m√¥ h√¨nh `y ~ x - 1` v√†
`y ~ x` khi x l√† categorical variable. Predictions c√≥ gi·ªëng nhau kh√¥ng
v√† t·∫°i sao?

**ƒê√°p √°n:**

M√¥ h√¨nh y \~ x - 1 kh√¥ng c√≥ intercept, m·ªói m·ª©c c·ªßa bi·∫øn ph√¢n lo·∫°i x c√≥
h·ªá s·ªë ri√™ng th·ªÉ hi·ªán gi√° tr·ªã trung b√¨nh t·ª´ng nh√≥m.

M√¥ h√¨nh y \~ x c√≥ intercept, c√°c h·ªá s·ªë c·ªßa m·ª©c x ƒë∆∞·ª£c t√≠nh so v·ªõi m·ª©c
tham chi·∫øu.

D·ª± ƒëo√°n kh√°c nhau: v·ªõi y \~ x - 1, d·ª± ƒëo√°n l√† gi√° tr·ªã trung b√¨nh nh√≥m;
v·ªõi y \~ x, d·ª± ƒëo√°n l√† intercept c·ªông h·ªá s·ªë c·ªßa nh√≥m, v·ªÅ c∆° b·∫£n t∆∞∆°ng
ƒë∆∞∆°ng nh∆∞ng bi·ªÉu di·ªÖn kh√°c.

**C√¢u 2: (10 ƒëi·ªÉm)** Trong model matrix c·ªßa `x1 * x2` (sim3), gi·∫£i th√≠ch
√Ω nghƒ©a c·ªßa c√°c c·ªôt `x1:x2b`, `x1:x2c`, `x1:x2d`. T·∫°i sao interaction
term quan tr·ªçng?

**ƒê√°p √°n:**

Trong model matrix c·ªßa x1 \* x2, c√°c c·ªôt x1:x2b, x1:x2c, x1:x2d l√† c√°c
t∆∞∆°ng t√°c gi·ªØa bi·∫øn x1 v√† t·ª´ng m·ª©c c·ªßa bi·∫øn ph√¢n lo·∫°i x2 (ngo·∫°i tr·ª´ m·ª©c
tham chi·∫øu).

√ù nghƒ©a: th·ªÉ hi·ªán ·∫£nh h∆∞·ªüng c·ªßa x1 ph·ª• thu·ªôc v√†o t·ª´ng nh√≥m c·ªßa x2.

T∆∞∆°ng t√°c quan tr·ªçng v√¨ n√≥ cho ph√©p m√¥ h√¨nh n·∫Øm b·∫Øt hi·ªáu ·ª©ng k·∫øt h·ª£p
kh√¥ng ƒë∆°n gi·∫£n ch·ªâ l√† t·ªïng c√°c bi·∫øn m√† thay ƒë·ªïi theo nh√≥m.

**C√¢u 3: (10 ƒëi·ªÉm)** D·ª±a v√†o k·∫øt qu·∫£ ph√¢n t√≠ch sim4, m√¥ h√¨nh n√†o t·ªët h∆°n
gi·ªØa `y ~ x1 + x2` v√† `y ~ x1 * x2`? S·ª≠ d·ª•ng nh·ªØng ti√™u ch√≠ n√†o ƒë·ªÉ ƒë√°nh
gi√°?

**ƒê√°p √°n:**

So s√°nh hai m√¥ h√¨nh y \~ x1 + x2 v√† y \~ x1 \* x2 d·ª±a tr√™n:

1.  Gi√° tr·ªã AIC ho·∫∑c BIC, m√¥ h√¨nh n√†o nh·ªè h∆°n th√¨ t·ªët h∆°n.

2.  Adjusted R-squared, gi√° tr·ªã cao h∆°n nghƒ©a l√† m√¥ h√¨nh gi·∫£i th√≠ch bi·∫øn
    ƒë·ªông t·ªët h∆°n.

3.  Ki·ªÉm ƒë·ªãnh ANOVA xem t∆∞∆°ng t√°c c√≥ √Ω nghƒ©a th·ªëng k√™ kh√¥ng.

M√¥ h√¨nh c√≥ t∆∞∆°ng t√°c (y \~ x1 \* x2) t·ªët h∆°n n·∫øu t∆∞∆°ng t√°c gi√∫p c·∫£i
thi·ªán ƒë√°ng k·ªÉ ƒë·ªô ph√π h·ª£p c·ªßa m√¥ h√¨nh.

## B√†i t·∫≠p th·ª±c h√†nh

### B√†i t·∫≠p 1: T·∫°o v√† ph√¢n t√≠ch m√¥ h√¨nh c·ªßa ri√™ng b·∫°n (10 ƒëi·ªÉm)

```{r}
# 1. T·∫°o d·ªØ li·ªáu m√¥ ph·ªèng v·ªõi noise kh√°c nhau
set.seed(123)
my_data <- tibble(
  x = seq(1, 20, by = 0.5),
  y_linear = 2 * x + 5 + rnorm(length(x), sd = 2),
  y_nonlinear = 2 * x + 0.1 * x^2 - 10 + rnorm(length(x), sd = 3)
)
```

# 2. X√¢y d·ª±ng m√¥ h√¨nh linear cho c·∫£ hai bi·∫øn y

```{r}
# 2. X√¢y d·ª±ng m√¥ h√¨nh linear cho c·∫£ hai bi·∫øn y
model_linear <- lm(y_linear ~ x, data = my_data)
model_nonlinear <- lm(y_nonlinear ~ x, data = my_data)

# 3. T·ªïng quan k·∫øt qu·∫£ m√¥ h√¨nh
summary(model_linear)
summary(model_nonlinear)

# 4. V·∫Ω d·ªØ li·ªáu v√† ƒë∆∞·ªùng h·ªìi quy

# Bi·ªÉu ƒë·ªì cho y_linear
p1 <- ggplot(my_data, aes(x = x, y = y_linear)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggtitle("Linear model: y_linear ~ x")

# Bi·ªÉu ƒë·ªì cho y_nonlinear
p2 <- ggplot(my_data, aes(x = x, y = y_nonlinear)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("Linear model: y_nonlinear ~ x")

print(p1)
print(p2)

# 5. Nh·∫≠n x√©t ng·∫Øn
cat("M√¥ h√¨nh linear v·ªõi y_linear cho k·∫øt qu·∫£ t·ªët h∆°n do y_linear g·∫ßn v·ªõi d·∫°ng tuy·∫øn t√≠nh.\n")
cat("M√¥ h√¨nh linear v·ªõi y_nonlinear kh√¥ng ph√π h·ª£p t·ªët do y_nonlinear c√≥ th√†nh ph·∫ßn phi tuy·∫øn.\n")

```

### Ph·∫ßn 1: Tr·ª±c quan h√≥a d·ªØ li·ªáu (2 ƒëi·ªÉm)

-   V·∫Ω scatter plot cho c·∫£ y_linear v√† y_nonlinear theo x

-   S·ª≠ d·ª•ng facet_wrap() ho·∫∑c facet_grid() ƒë·ªÉ hi·ªÉn th·ªã 2 bi·ªÉu ƒë·ªì c·∫°nh
    nhau

-   ƒê·∫∑t ti√™u ƒë·ªÅ v√† nh√£n tr·ª•c r√µ r√†ng

```{r}
my_data_long <- my_data %>%
  pivot_longer(cols = starts_with("y"), names_to = "type", values_to = "y")

ggplot(my_data_long, aes(x = x, y = y)) +
  geom_point(color = "gray20") +
  facet_wrap(~type, scales = "free_y") +
  labs(
    title = "Bi·ªÉu ƒë·ªì scatter c·ªßa y_linear v√† y_nonlinear",
    x = "x",
    y = "Gi√° tr·ªã y"
  ) +
  theme_minimal()

```

### **Ph·∫ßn 2: X√¢y d·ª±ng m√¥ h√¨nh Linear (2 ƒëi·ªÉm)**

-   X√¢y d·ª±ng m√¥ h√¨nh lm() cho y_linear \~ x

-   X√¢y d·ª±ng m√¥ h√¨nh lm() cho y_nonlinear \~ x

-   In ra summary c·ªßa c·∫£ 2 m√¥ h√¨nh

-   Nh·∫≠n x√©t v·ªÅ R-squared v√† c√°c h·ªá s·ªë h·ªìi quy

```{r}
model_linear <- lm(y_linear ~ x, data = my_data)
model_nonlinear <- lm(y_nonlinear ~ x, data = my_data)

summary(model_linear)
summary(model_nonlinear)

```

**Nh·∫≠n x√©t:**

-   model_linear c√≥ R-squared r·∫•t cao (\~0.98), cho th·∫•y m√¥ h√¨nh tuy·∫øn
    t√≠nh fit r·∫•t t·ªët d·ªØ li·ªáu tuy·∫øn t√≠nh.

-   model_nonlinear c√≥ R-squared th·∫•p h∆°n ƒë√°ng k·ªÉ (kho·∫£ng 0.9 ho·∫∑c nh·ªè
    h∆°n), v√¨ d·ªØ li·ªáu th·∫≠t c√≥ quan h·ªá phi tuy·∫øn (c√≥ th√†nh ph·∫ßn x¬≤).

### **Ph·∫ßn 3: Th√™m predictions v√† residuals (2 ƒëi·ªÉm)**

-   S·ª≠ d·ª•ng add_predictions() ƒë·ªÉ th√™m gi√° tr·ªã d·ª± ƒëo√°n

-   S·ª≠ d·ª•ng add_residuals() ƒë·ªÉ th√™m residuals

-   T·∫°o dataframe ch·ª©a c·∫£ predictions v√† residuals c·ªßa c·∫£ 2 m√¥ h√¨nh

```{r}
my_data_extended <- my_data %>%
  add_predictions(model_linear, var = "pred_linear") %>%
  add_residuals(model_linear, var = "resid_linear") %>%
  add_predictions(model_nonlinear, var = "pred_nonlinear") %>%
  add_residuals(model_nonlinear, var = "resid_nonlinear")

head(my_data_extended)

```

### **Ph·∫ßn 4: V·∫Ω bi·ªÉu ƒë·ªì v·ªõi ƒë∆∞·ªùng h·ªìi quy (2 ƒëi·ªÉm)**

-   V·∫Ω scatter plot v·ªõi ƒëi·ªÉm d·ªØ li·ªáu g·ªëc

-   Th√™m ƒë∆∞·ªùng h·ªìi quy t·ª´ m√¥ h√¨nh (s·ª≠ d·ª•ng geom_line() v·ªõi predictions)

-   T·∫°o 2 bi·ªÉu ƒë·ªì ri√™ng bi·ªát cho y_linear v√† y_nonlinear

-   So s√°nh ƒë·ªô fit c·ªßa m√¥ h√¨nh v·ªõi d·ªØ li·ªáu

-   ƒê·ªãnh d·∫°ng:

-   M√†u ƒë·ªè cho ƒë∆∞·ªùng h·ªìi quy

-   ƒêi·ªÉm d·ªØ li·ªáu m√†u ƒëen ho·∫∑c x√°m

-   S·ª≠ d·ª•ng facet_wrap() ƒë·ªÉ t√°ch bi·ªát 2 tr∆∞·ªùng h·ª£p

```{r}
my_data_long_pred <- my_data_extended %>%
  pivot_longer(
    cols = c(y_linear, y_nonlinear, pred_linear, pred_nonlinear),
    names_to = c(".value", "type"),
    names_pattern = "(y|pred)_(.*)"
  )

ggplot(my_data_long_pred, aes(x = x)) +
  geom_point(aes(y = y), color = "black") +
  geom_line(aes(y = pred, color = "H·ªìi quy (Linear model)"), size = 1) +
  facet_wrap(~type, scales = "free_y") +
  scale_color_manual(values = c("H·ªìi quy (Linear model)" = "red")) +
  labs(
    title = "Bi·ªÉu ƒë·ªì d·ªØ li·ªáu v√† ƒë∆∞·ªùng h·ªìi quy",
    x = "x",
    y = "Gi√° tr·ªã y"
  ) +
  theme_minimal()

```

**Nh·∫≠n x√©t:**

-   V·ªõi y_linear, ƒë∆∞·ªùng h·ªìi quy kh·ªõp r·∫•t s√°t d·ªØ li·ªáu.

-   V·ªõi y_nonlinear, ƒë∆∞·ªùng h·ªìi quy kh√¥ng m√¥ t·∫£ ƒë∆∞·ª£c ƒë·ªô cong c·ªßa d·ªØ li·ªáu.

### **Ph·∫ßn 5: Ph√¢n t√≠ch Residuals (1 ƒëi·ªÉm)**

-   V·∫Ω residual plot (x vs residuals) cho c·∫£ 2 m√¥ h√¨nh

-   Th√™m ƒë∆∞·ªùng tham chi·∫øu geom_hline(yintercept = 0) m√†u ƒë·ªè

-   Nh·∫≠n x√©t v·ªÅ pattern c·ªßa residuals

```{r}
resid_long <- my_data_extended %>%
  select(x, resid_linear, resid_nonlinear) %>%
  pivot_longer(cols = starts_with("resid"), names_to = "model", values_to = "resid")

ggplot(resid_long, aes(x = x, y = resid)) +
  geom_point(color = "gray20") +
  geom_hline(yintercept = 0, color = "red") +
  facet_wrap(~model, scales = "free_y") +
  labs(
    title = "Residual plots cho hai m√¥ h√¨nh",
    x = "x",
    y = "Residuals"
  ) +
  theme_minimal()

```

### **C√¢u h·ªèi:**

1.  M√¥ h√¨nh n√†o c√≥ residuals ph√¢n b·ªë t·ªët h∆°n?

```         
M√¥ h√¨nh y_linear c√≥ residuals ph√¢n b·ªë ng·∫´u nhi√™n quanh 0, t·ªët h∆°n.
```

2.  C√≥ th·∫•y pattern n√†o trong residuals c·ªßa model_nonlinear kh√¥ng?

```         
C√≥ d·∫°ng cong (u-shape).
```

3.  Pattern ƒë√≥ cho bi·∫øt ƒëi·ªÅu g√¨ v·ªÅ m√¥ h√¨nh?

```         
M√¥ h√¨nh tuy·∫øn t√≠nh kh√¥ng b·∫Øt ƒë∆∞·ª£c m·ªëi quan h·ªá phi tuy·∫øn, n√™n residuals th·ªÉ hi·ªán xu h∆∞·ªõng c√≥ h·ªá th·ªëng.
```

### **Ph·∫ßn 6: So s√°nh ch·∫•t l∆∞·ª£ng m√¥ h√¨nh (1 ƒëi·ªÉm)**

1.  T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°:

-   Mean Absolute Error (MAE): mean(abs(resid))

-   Root Mean Squared Error (RMSE): sqrt(mean(resid\^2))

-   Standard deviation c·ªßa residuals

```{r}
mae_linear <- mean(abs(resid(model_linear)))
mae_nonlinear <- mean(abs(resid(model_nonlinear)))

rmse_linear <- sqrt(mean(resid(model_linear)^2))
rmse_nonlinear <- sqrt(mean(resid(model_nonlinear)^2))

sd_linear <- sd(resid(model_linear))
sd_nonlinear <- sd(resid(model_nonlinear))
```

**Nh·∫≠n x√©t:**

-   M√¥ h√¨nh `y_linear` c√≥ MAE v√† RMSE th·∫•p h∆°n r√µ r·ªát ‚Üí d·ª± ƒëo√°n ch√≠nh
    x√°c h∆°n.

-   `y_nonlinear` c√≥ sai s·ªë cao h∆°n do ch∆∞a m√¥ t·∫£ ƒë√∫ng d·∫°ng d·ªØ li·ªáu.

2.  T·∫°o b·∫£ng so s√°nh 2 m√¥ h√¨nh

```{r}
comparison <- tibble(
  Model = c("y_linear", "y_nonlinear"),
  MAE = c(mae_linear, mae_nonlinear),
  RMSE = c(rmse_linear, rmse_nonlinear),
  SD_Resid = c(sd_linear, sd_nonlinear)
)

comparison
```

### **Ph·∫ßn 7: C·∫£i thi·ªán m√¥ h√¨nh phi tuy·∫øn (Bonus)**

-   X√¢y d·ª±ng m√¥ h√¨nh polynomial cho y_nonlinear: lm(y_nonlinear \~ x +
    I(x\^2))

-   So s√°nh v·ªõi m√¥ h√¨nh linear ƒë∆°n gi·∫£n

-   V·∫Ω c·∫£ 3 ƒë∆∞·ªùng: d·ªØ li·ªáu g·ªëc, linear model, polynomial model

-   Nh·∫≠n x√©t v·ªÅ s·ª± c·∫£i thi·ªán

```{r}
model_poly <- lm(y_nonlinear ~ x + I(x^2), data = my_data)
summary(model_poly)

my_data_poly <- my_data %>%
  add_predictions(model_poly, var = "pred_poly")

ggplot(my_data_poly, aes(x = x)) +
  geom_point(aes(y = y_nonlinear), color = "black") +
  geom_line(aes(y = pred_poly, color = "Polynomial (x + x¬≤)"), size = 1.2) +
  geom_line(aes(y = pred_nonlinear, color = "Linear model"), data = my_data_extended) +
  scale_color_manual(values = c("Polynomial (x + x¬≤)" = "blue", "Linear model" = "red")) +
  labs(
    title = "So s√°nh m√¥ h√¨nh Linear v√† Polynomial cho y_nonlinear",
    x = "x",
    y = "Gi√° tr·ªã y"
  ) +
  theme_minimal()

```

### **C√¢u h·ªèi**

1.  T·∫°i sao m√¥ h√¨nh linear kh√¥ng fit t·ªët v·ªõi y_nonlinear?

```         
V√¨ quan h·ªá gi·ªØa y_nonlinear v√† x l√† phi tuy·∫øn (c√≥ th√†nh ph·∫ßn b·∫≠c 2), m√¥ h√¨nh tuy·∫øn t√≠nh kh√¥ng m√¥ t·∫£ ƒë∆∞·ª£c ƒë·ªô cong ƒë√≥.
```

2.  L√†m th·∫ø n√†o ƒë·ªÉ ph√°t hi·ªán m·ªôt m√¥ h√¨nh kh√¥ng ph√π h·ª£p th√¥ng qua
    residual plot?

```         
Khi residuals kh√¥ng ph√¢n b·ªë ng·∫´u nhi√™n quanh 0 m√† c√≥ pattern (v√≠ d·ª•: cong, tƒÉng d·∫ßn, gi·∫£m d·∫ßn theo x). 
```

3.  Khi n√†o n√™n s·ª≠ d·ª•ng polynomial regression thay v√¨ linear regression?

```         
Khi m·ªëi quan h·ªá gi·ªØa bi·∫øn ƒë·ªôc l·∫≠p v√† ph·ª• thu·ªôc c√≥ d·∫°ng cong (nonlinear), kh√¥ng th·ªÉ m√¥ t·∫£ t·ªët b·∫±ng ƒë∆∞·ªùng th·∫≥ng.
```

## K·∫øt lu·∫≠n

Trong b√†i lab n√†y, ch√∫ng ta ƒë√£ h·ªçc ƒë∆∞·ª£c:

-   C√°ch x√¢y d·ª±ng v√† ƒë√°nh gi√° m√¥ h√¨nh tuy·∫øn t√≠nh
-   ·∫¢nh h∆∞·ªüng c·ªßa outliers ƒë·∫øn m√¥ h√¨nh
-   C√°c ph∆∞∆°ng ph√°p ƒëo kho·∫£ng c√°ch kh√°c nhau
-   C√°ch tr·ª±c quan h√≥a m√¥ h√¨nh v√† residuals
-   S·ª≠ d·ª•ng c√°c c√¥ng c·ª• h·ªó tr·ª£ trong package `modelr`
-   L√†m vi·ªác v·ªõi d·ªØ li·ªáu categorical v√† interaction terms

Nh·ªØng k·ªπ nƒÉng n√†y l√† n·ªÅn t·∫£ng quan tr·ªçng cho vi·ªác ph√¢n t√≠ch d·ªØ li·ªáu v√†
machine learning trong R.
